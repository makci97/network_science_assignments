{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Assignment — Graph Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment lines in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install node2vec\n",
    "#!pip install dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.data import CoraGraphDataset\n",
    "from dgl.nn import GraphConv, GATConv, SAGEConv\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. The baseline — Node2Vec (2 points)\n",
    "\n",
    "We will use Node2Vec as a baseline for comparison with other models. Let us download the Cora dataset, that consists of scientific publications classified into one of seven classes. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CoraGraphDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert into Networkx and numpy types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = dgl.to_networkx(data[0])\n",
    "labels = data[0].ndata['label'].numpy()\n",
    "train_mask = data[0].ndata['train_mask'].numpy()\n",
    "test_mask = data[0].ndata['test_mask'].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute probabilities and generate random walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2v = Node2Vec(G, dimensions=16, walk_length=20, \n",
    "               num_walks=10, p=1, q=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we fit our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = n2v.fit(window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `n2v_train_test` that takes a `model`, graph `G`, np.arrays `labels`, `train_mask` and `test_mask`. The function returns np.arrays with embeddings `X_train`, `X_test` and np.arrays with labels `y_train`, `y_test`. The embeddings are in the `model.wv attribute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e90318e20bb3f462b99b6e027f1193d8",
     "grade": false,
     "grade_id": "cell-aee487348c53f2bf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def n2v_train_test(model, G, labels, train_mask, test_mask):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e99bf7f00e7760c051255abf8951043",
     "grade": true,
     "grade_id": "cell-428f179af4977d49",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = n2v_train_test(model, G, labels, train_mask, test_mask)\n",
    "assert X_train.shape == (140, 16)\n",
    "assert X_test.shape == (1000, 16)\n",
    "assert y_train.shape == (140,)\n",
    "assert y_test.shape == (1000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `logreg` that takes `X_train`, `X_test`, `y_train`, `y_test` and returns predictions from LogisticRegression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8dc00c62fac3be3b172f359cb4061f6",
     "grade": false,
     "grade_id": "cell-f6eb0d80de3359f9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def logreg(X_train, X_test, y_train, y_test):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cdf54628442dfe290b810cc80f05f58f",
     "grade": true,
     "grade_id": "cell-3e4e33833d3b9460",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = logreg(X_train, X_test, y_train, y_test)\n",
    "score = balanced_accuracy_score(y_test, y_pred)\n",
    "assert score > 0.55\n",
    "print('Balanced accuracy: {:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch quick start\n",
    "\n",
    "Here are the basics of PyTourch. If you are familiar with this material, skip it. \n",
    "\n",
    "First, let us generate a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "X = np.linspace(-10, 10, N)\n",
    "np.random.seed(0)\n",
    "y = np.sin(X) + np.random.normal(0, 0.2, N)\n",
    "X = minmax_scale(X, (-1, 1))\n",
    "y = minmax_scale(y, (-1, 1))\n",
    "plt.scatter(X, y, s=3)\n",
    "plt.ylim(-1.5, 1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert np.arrays into tensors via `torch.FloatTensor` (float type) or `torch.LongTensor` (int type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.FloatTensor(X[:, None])\n",
    "y_tensor = torch.FloatTensor(y[:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple shallow perceptron with layer sizes $1 \\to 64 \\to 64 \\to 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(1, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = nn.MSELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train loop constists of: computing loss, grad, making a step to the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 300\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    y_pred = model.forward(X_tensor)\n",
    "    loss = MSE(y_pred, y_tensor)\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    plt.scatter(X, y, s=3)\n",
    "    plt.plot(X, y_pred.detach().numpy(), linewidth=2, c='tab:orange')\n",
    "    plt.title('Epoch: {}/{}, Loss: {:.4f}'.format(i+1, n_epochs, loss.item()))\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Spectral graph convolutional network (3 points)\n",
    "\n",
    "Graph convolution mathematically is defined as follows:\n",
    "\n",
    "$$h_i^{(l+1)} = \\sigma(b^{(l)} + \\sum_{j\\in\\mathcal{N}(i)}\\frac{1}{c_{ij}}h_j^{(l)}W^{(l)})$$\n",
    "\n",
    "where $\\mathcal{N}(i)$ is the set of neighbors of node $i$,$c_{ij}$ is the product of the square root of node degrees (i.e.,  $c_{ij} = \\sqrt{|\\mathcal{N}(i)|}\\sqrt{|\\mathcal{N}(j)|}$), and $\\sigma$ is an activation function.\n",
    "\n",
    "Build a cora dataset again, but without conversion into numpy or networkx. These are tensors by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CoraGraphDataset()\n",
    "G = data[0]\n",
    "features = G.ndata['feat']\n",
    "labels = G.ndata['label']\n",
    "train_mask = G.ndata['train_mask']\n",
    "test_mask = G.ndata['test_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an one-hot encoded feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And labels of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero in-degree nodes will lead to invalid output value. This is because no message will be passed to those nodes, the aggregation function will be appied on empty input. A common practice to avoid this is to add a self-loop for each node in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = dgl.add_self_loop(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete a class `GCN`. Use `GraphConv` layers with sizes $1433 \\to 16 \\to 7$. Use `F.relu` activation after the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e703f6a06efb0127b962be9a85a57cb",
     "grade": false,
     "grade_id": "cell-5e0c138f5ccaea0e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        #self.conv1 = <YOUR CODE>\n",
    "        #self.conv2 = <YOUR CODE>\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, G, features):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fb7b8435983f851fed960d45026777b",
     "grade": true,
     "grade_id": "cell-fe037acfca3092bc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = GCN()\n",
    "assert str(model) == 'GCN(\\n  (conv1): GraphConv(in=1433, out=16, normalization=both, activation=None)\\n  (conv2): GraphConv(in=16, out=7, normalization=both, activation=None)\\n)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification tasks we will use `CrossEntropy` loss. Change an optimizer if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrossEntropy = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the train loop. To speed up calculation test loss, use `torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47d98a828c1bc00a90ed3b20d2bfe226",
     "grade": false,
     "grade_id": "cell-859836ad344792e3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "\n",
    "log = []\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    #train_loss = <YOUR CODE>\n",
    "    #test_loss = <YOUR CODE>\n",
    "    \n",
    "    log.append([train_loss, test_loss])\n",
    "    \n",
    "    plt.plot(np.array(log))\n",
    "    plt.title('Epoch: {}/{}'.format(i+1, n_epochs))\n",
    "    plt.legend(['train', 'test'])\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "157a0f072f99b183169529a26e04f1e4",
     "grade": true,
     "grade_id": "cell-0dbd2da971a339fe",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "logits = model.forward(G, features)\n",
    "y_pred = torch.argmax(logits[test_mask], 1)\n",
    "score = balanced_accuracy_score(labels[test_mask], y_pred)\n",
    "assert score > 0.75\n",
    "print('Balanced accuracy: {:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Graph attention network (3 points)\n",
    "\n",
    "Let us apply Graph Attention Network over an input signal.\n",
    "\n",
    "$$h_i^{(l+1)} = \\sum_{j\\in \\mathcal{N}(i)} \\alpha_{i,j} W^{(l)} h_j^{(l)}$$\n",
    "\n",
    "where $\\alpha_{ij}$ is the attention score bewteen node $i$ and node $j$:\n",
    "\n",
    "$$\\alpha_{ij}^{l} = \\text{Softmax}_{i} (e_{ij}^{l})$$\n",
    "\n",
    "$$e_{ij}^{l} = \\text{LeakyReLU}\\left(\\vec{a}^T [W h_{i} \\| W h_{j}]\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CoraGraphDataset()\n",
    "G = data[0]\n",
    "G = dgl.add_self_loop(G)\n",
    "features = G.ndata['feat']\n",
    "labels = G.ndata['label']\n",
    "train_mask = G.ndata['train_mask']\n",
    "test_mask = G.ndata['test_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete a class `GAT`. Use `GATConv` layers:\n",
    "`GATConv(1433, 8, num_heads=8)` $\\to$ `GATConv(8*8, 7, num_heads=1)`\n",
    "\n",
    "In `forward` use \n",
    "* `tensor.view(-1, m * n)` to reshape `(n_features, n_heads, out_dim) -> (n_features, n_heads * out_dim)`\n",
    "* `F.elu` activation after concat\n",
    "* `tensor.squeeze` to decrease shape `(n_features, 1, out_dim) -> (n_features, out_dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cdc8345eb40be185359aeb01fff87fde",
     "grade": false,
     "grade_id": "cell-2ac8340166410ded",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        #self.conv1 = <YOUR CODE>\n",
    "        #self.conv2 = <YOUR CODE>\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, G, features):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f46ec48603ff02a5aedabf02176194a",
     "grade": true,
     "grade_id": "cell-8b1838d547caf32d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = GAT()\n",
    "assert str(model) == 'GAT(\\n  (conv1): GATConv(\\n    (fc): Linear(in_features=1433, out_features=64, bias=False)\\n    (feat_drop): Dropout(p=0.0, inplace=False)\\n    (attn_drop): Dropout(p=0.0, inplace=False)\\n    (leaky_relu): LeakyReLU(negative_slope=0.2)\\n  )\\n  (conv2): GATConv(\\n    (fc): Linear(in_features=64, out_features=7, bias=False)\\n    (feat_drop): Dropout(p=0.0, inplace=False)\\n    (attn_drop): Dropout(p=0.0, inplace=False)\\n    (leaky_relu): LeakyReLU(negative_slope=0.2)\\n  )\\n)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrossEntropy = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), \n",
    "                       lr=0.005, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the train loop. To speed up calculation test loss, use `torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d409df1ea0e998208b32f57cfb9b814",
     "grade": false,
     "grade_id": "cell-8d019b9371d11f59",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 150\n",
    "\n",
    "log = []\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    #train_loss = <YOUR CODE>\n",
    "    #test_loss = <YOUR CODE>\n",
    "    \n",
    "    log.append([train_loss, test_loss])\n",
    "    \n",
    "    plt.plot(np.array(log))\n",
    "    plt.title('Epoch: {}/{}'.format(i+1, n_epochs))\n",
    "    plt.legend(['train', 'test'])\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3b5d5976d418d83fb7ca869601a8f74",
     "grade": true,
     "grade_id": "cell-f0b5cbfec9b4510d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "logits = model.forward(G, features)\n",
    "y_pred = torch.argmax(logits[test_mask], 1)\n",
    "score = balanced_accuracy_score(labels[test_mask], y_pred)\n",
    "assert score > 0.7\n",
    "print('Balanced accuracy: {:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5. GraphSAGE (2 points)\n",
    "\n",
    "Consider GraphSAGE, a representation learning technique suitable for dynamic graphs. GraphSAGE is capable of predicting embedding of a new node, without requiring a re-training procedure. To do so, GraphSAGE learns aggregator functions that can induce the embedding of a new node given its features and neighborhood. This is called inductive learning.\n",
    "\n",
    "$$h_{\\mathcal{N}(i)}^{(l+1)} = \\mathrm{aggregate}\\left(\\{h_{j}^{l}, \\forall j \\in \\mathcal{N}(i) \\}\\right)$$\n",
    "$$h_{i}^{(l+1)} = \\sigma \\left(W \\cdot \\text{concat}(h_{i}^{l}, h_{\\mathcal{N}(i)}^{l+1}) \\right)$$\n",
    "$$h_{i}^{(l+1)} = \\mathrm{norm}(h_{i}^{l})$$\n",
    "\n",
    "Aggregator types here can be `mean`, `gcn`, `pool`, `lstm`. Consider GraphSAGE on the Karate Club graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()\n",
    "\n",
    "labels = [1 if i=='Mr. Hi' else 0 for i in nx.get_node_attributes(G, 'club').values()]\n",
    "labels = torch.LongTensor(labels)\n",
    "features = torch.FloatTensor(np.arange(0, 34)[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(\n",
    "    G, with_labels=True, \n",
    "    node_color=['tab:orange' if i==1 else 'tab:blue' for i in labels], \n",
    "    cmap=plt.cm.tab10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us delete a node 31, train a model and then return it and predict its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(34)\n",
    "idx = idx[idx != 31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New labels and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels[idx]\n",
    "features = features[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(\n",
    "    G.subgraph(idx), with_labels=True, \n",
    "    node_color=['tab:orange' if i==1 else 'tab:blue' for i in labels], \n",
    "    cmap=plt.cm.tab10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let choose test and train nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = [31, 32, 0, 11, 13, 2, 23, 29, 8]\n",
    "train_idx = list(set(np.arange(33)).difference(test_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the graph, test nodes are gray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_color = np.ones((33, 3))\n",
    "node_color[labels == 0] = plt.cm.tab10(0)[:3]\n",
    "node_color[labels == 1] = plt.cm.tab10(1)[:3]\n",
    "node_color[test_idx] = (0.9, 0.9, 0.9)\n",
    "\n",
    "nx.draw_kamada_kawai(G.subgraph(idx), with_labels=True, \n",
    "                     node_color=node_color, cmap=plt.cm.tab10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a dgl graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_graph = dgl.from_networkx(G.subgraph(idx))\n",
    "initial_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete a class `SAGE`. Use `SAGEConv` layers with sizes $1 \\to 16 \\to 2$ and `mean` aggregation function. Put `F.relu` activation after the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2c8612507322767a89dd70bc39a53e4",
     "grade": false,
     "grade_id": "cell-c83ca7ab5833bc2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.conv1 = <YOUR CODE>\n",
    "        #self.conv2 = <YOUR CODE>\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "474bc258f2a7ddf38af45b1d609f05c6",
     "grade": true,
     "grade_id": "cell-a7513e546c544396",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = SAGE()\n",
    "assert str(model) == 'SAGE(\\n  (conv1): SAGEConv(\\n    (feat_drop): Dropout(p=0.0, inplace=False)\\n    (fc_self): Linear(in_features=1, out_features=16, bias=True)\\n    (fc_neigh): Linear(in_features=1, out_features=16, bias=True)\\n  )\\n  (conv2): SAGEConv(\\n    (feat_drop): Dropout(p=0.0, inplace=False)\\n    (fc_self): Linear(in_features=16, out_features=2, bias=True)\\n    (fc_neigh): Linear(in_features=16, out_features=2, bias=True)\\n  )\\n)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrossEntropy = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 150\n",
    "\n",
    "log = []\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    logits = model.forward(initial_graph, features)\n",
    "    loss = CrossEntropy(logits[train_idx], labels[train_idx])\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model.forward(initial_graph, features)\n",
    "        test_loss = CrossEntropy(logits[test_idx], labels[test_idx])\n",
    "    \n",
    "    log.append([loss.item(), test_loss.item()])\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(np.array(log))\n",
    "    plt.title('Epoch: {}/{}'.format(i+1, n_epochs))\n",
    "    plt.legend(['train', 'test'])\n",
    "    \n",
    "    y_pred = torch.argmax(logits, 1)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    nx.draw_kamada_kawai(\n",
    "        G.subgraph(idx), with_labels=True, \n",
    "        node_color=['tab:orange' if i==1 else 'tab:blue' for i in y_pred], \n",
    "        cmap=plt.cm.tab10)\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "    clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check that prediction for the node 31 is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e02fe6505e7d0fce34e4e0f675c1519",
     "grade": true,
     "grade_id": "cell-41e44fe2c4bf18a1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "graph = dgl.from_networkx(G)\n",
    "labels = [1 if i=='Mr. Hi' else 0 for i in nx.get_node_attributes(G, 'club').values()]\n",
    "labels = torch.LongTensor(labels)\n",
    "features = torch.FloatTensor(np.arange(0, 34)[:, None])\n",
    "predictions = torch.argmax(model(graph, features), 1)\n",
    "assert predictions[31] == labels[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
